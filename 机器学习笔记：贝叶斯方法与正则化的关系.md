# 贝叶斯方法与正则化

统计学分为两个学派：频率派和贝叶斯派。

## 频率派

频率派常用的参数估计方法为**极大似然法**（MLE），它的目标是让似然函数最大化，就是求出一个固定参数，这个参数使数据出现的概率最大。

假设数据采样分布为$p(x;\theta)$，即参数为$\theta$时，样本$x$出现的概率。假设现在观测到一组数据$x_1,x_2,\cdots,x_n$，数据之间是独立同分布，则这组数据出现的概率可表示为：

$$L(\theta)=L(x_1,x_2,\cdots,x_n;\theta)=\prod_{i=1}^np(x_i;\theta)$$

$L(\theta)$称为似然函数，注意$p(x;\theta)$不是似然函数，$L(x_1,x_2,\cdots,x_n;\theta)$才是似然函数。

极大似然法就是求解使$L(\theta)$最大的$\theta$，这等效于一个最优化问题。

## 贝叶斯派

贝叶斯派常用的参数估计方法为**最大后验估计**（MAP）,它以贝叶斯公式作为基础。

$$P(H|D)=\frac{P(D|H)*P(H)}{P(D)}$$

式中$P(H)$称为先验概率，$P(D|H)$称为似然函数，$P(D)$称为证据，$P(H|D)$称为后验概率。

先验概率是根据以往经验和分析得到的概率，可以视为$H$的初始可信程度（贝叶斯派眼中的概率是对事物的主观的可信程度），数据$D$会作为证据出现，将数据纳入考虑范围后，$H$的初始概率会被更新，新的概率就是$H$的后验概率。

贝叶斯公式在求$H$的概率时除了根据数据$D$，还考虑到了$H$的历史经验。这一做法和频率派不同，频率派只考虑数据$D$。

$$p(\theta;x_1,x_2,\cdots,x_n) = \frac{p(x_1,x_2,\cdots,x_n;\theta)*p(\theta)}{p(x_1,x_2,\cdots,x_n)}$$

由于分母是常量，所以

$$p(\theta;x_1,x_2,\cdots,x_n) \propto \prod_{i=1}^np(x_i;\theta)*p(\theta)$$

最大后验估计就是求解使$p(\theta;x_1,x_2,\cdots,x_n)$最大的$\theta$，这等效于一个最优化问题。

对比MLE和MAP发现，MAP比MLE多乘了一个先验概率$p(\theta)$，所以MAP综合考虑了数据和先验概率。

先验信息是在使用数据之前关于分析对象的已知知识，它容易受到主观因素影响。当已有的知识不足以形成先验信息时，贝叶斯派引入了**无信息先验**，就是未知参数取到所有值的可能性都相等，即满足均匀分布，先验概率是一个常数，此时MAP和MLE是等效的。


## 正则化

**正则化可以对学习到的参数增加约束**，使之落在某个特定的范围内，其中L1正则化可以使参数具有稀疏性，L2正则化可以使参数聚拢在0值附近。

从贝叶斯派的角度来看，MLE其实也是有先验概率的，只不过它的先验分布是“未知参数取到所有值的可能性都相等”，**相当于没有对参数进行约束**。而MAP首先假设未知参数服从某特定分布，然后用数据来修正这个先验分布，**这个先验分布相当于对参数做了约束**。

所以贝叶斯方法与正则化都能够对参数做约束。在线性模型中，假定参数服从高斯分布，然后用MAP求解，与使用MLE增加L2正则化来求解，效果是等价的。

